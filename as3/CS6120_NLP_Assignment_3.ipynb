{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6120 NLP Fall 2023 Assignment 3\n",
    "\n",
    "## Implementing Skipgram and CBOW Algorithms\n",
    "\n",
    "### Background: \n",
    "Word embeddings are dense vector representations of words in a continuous vector space. Skipgram and CBOW are two primary algorithms introduced by Mikolov et al. in their 2013 papers that form the basis of the popular word2vec model. While both are used for generating word embeddings, they use different architectures and techniques.\n",
    "\n",
    "- Skipgram: Given a word, this model predicts the surrounding context words.\n",
    "- CBOW (Continuous Bag-of-Words): Given context words, this model predicts the target word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks:\n",
    "1. Data Collection:\n",
    "Download the text8 dataset, which is a cleaned version of the first 100MB of the English Wikipedia dump. It is available on several NLP data repositories.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Download the dataset\n",
    "url = \"http://mattmahoney.net/dc/text8.zip\"\n",
    "response = requests.get(url, allow_redirects = True)\n",
    "\n",
    "with open('text8.zip', 'wb') as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philosophy is the belief that rulers are unnecessary and should be abolished although there are differing interpretations of what this means anarchism also refers to related social movements that advocate the elimination of authoritarian institutions particularly the state the word anarchy as most anarchists use it does not imply chaos nihilism or anomie but rather a harmonious anti authoritarian society in place of what are regarded as authoritarian political structures and coercive economic instituti\n",
      "<class 'str'>\n",
      "The length of the `data` is 100000000\n"
     ]
    }
   ],
   "source": [
    "# Loading and preparing the text8 dataset\n",
    "with open('text8', 'r') as file:\n",
    "    data = file.read()\n",
    "    \n",
    "print(data[:1000])\n",
    "print(type(data))\n",
    "print('The length of the {} is {}' .format('`data`', len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Pre-processing:\n",
    "- Tokenize the dataset.\n",
    "- Remove stopwords and non-alphabetic tokens.\n",
    "- Build a vocabulary of the most frequent words (e.g., top 10,000 or 20,000 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import package\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from pprint import pprint\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import  word_tokenize\n",
    "import nltk\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "# add more packages\n",
    "from typing import Dict\n",
    "from typing import Set\n",
    "from typing import List\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data\n",
    "def clean_corpus(line: list[str]) -> list[str]:\n",
    "    '''\n",
    "    preprocess and clean a given line.\n",
    "\n",
    "    - line: The text line to be cleaned.\n",
    "    ---\n",
    "    - list: A list of preprocessed tokens from the line.\n",
    "    '''\n",
    "    tokens = word_tokenize(line)\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]  \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10888361\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['anarchism',\n",
       " 'originated',\n",
       " 'term',\n",
       " 'abuse',\n",
       " 'first',\n",
       " 'used',\n",
       " 'early',\n",
       " 'working',\n",
       " 'class',\n",
       " 'radicals']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing the data\n",
    "lines: list[list[str]] = []\n",
    "\n",
    "# Predefined list of stop words\n",
    "stop_words: set = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenize the text documents and update the lists word_list and lines\n",
    "lines = clean_corpus(data)\n",
    "print(len(lines))\n",
    "lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one',\n",
       " 'zero',\n",
       " 'nine',\n",
       " 'two',\n",
       " 'eight',\n",
       " 'five',\n",
       " 'three',\n",
       " 'four',\n",
       " 'six',\n",
       " 'seven']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a vocabulary of the most frequent words (e.g., top 10,000 or 20,000 words).\n",
    "word_counts = Counter(lines)\n",
    "top_freq_words = [word for word, count in word_counts.most_common(10000)]\n",
    "top_freq_words[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Implement CBOW:\n",
    "- Create the architecture for CBOW with an embedding layer and a linear layer.\n",
    "- Generate training samples. For each word in the dataset, use n surrounding words as context.\n",
    "- Train the model using a suitable optimizer and loss function.\n",
    "- Extract word embeddings for the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBOW is Continuous Bag Of Words, another vision of Word2Vec\n",
    "- Reference: https://www.youtube.com/watch?v=ghu_5o42QGQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the architecture for CBOW with an embedding layer and a linear layer.\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.linear = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, context_words):\n",
    "        embedded_words = self.embeddings(context_words)\n",
    "        # We average the embeddings across the context words (dim=0 since context words are in the first dimension)\n",
    "        avg_embedded = embedded_words.mean(dim = 0)\n",
    "        logits = self.linear(avg_embedded)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training samples. For each word in the dataset, use n surrounding words as context.\n",
    "def generate_context_pairs(corpus, window_size, vocab_set):\n",
    "    data = []\n",
    "    for i, word in enumerate(corpus):\n",
    "        if word not in vocab_set:\n",
    "            continue\n",
    "        context = [corpus[j] \n",
    "                   for j in list(range(max(0, i - window_size), i)) \n",
    "                   + list(range(i + 1, min(len(corpus), i + 1 + window_size))) \n",
    "                   if corpus[j] in vocab_set]\n",
    "        target = word\n",
    "        data.append((context, target))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# check GPU is work\n",
    "# Train the model using a suitable optimizer and loss function.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   3%|â–Ž         | 75111/2842521 [01:20<49:20, 934.88it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\GithubRepo\\NaturalLanguageProcessing_Intro\\as3\\CS6120_NLP_Assignment_3.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GithubRepo/NaturalLanguageProcessing_Intro/as3/CS6120_NLP_Assignment_3.ipynb#X20sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GithubRepo/NaturalLanguageProcessing_Intro/as3/CS6120_NLP_Assignment_3.ipynb#X20sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/GithubRepo/NaturalLanguageProcessing_Intro/as3/CS6120_NLP_Assignment_3.ipynb#X20sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GithubRepo/NaturalLanguageProcessing_Intro/as3/CS6120_NLP_Assignment_3.ipynb#X20sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mtotal_loss\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 100\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "window_size = 2\n",
    "\n",
    "# Preparing data\n",
    "vocab = set(top_freq_words)\n",
    "word2idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx2word = {i: word for word, i in word2idx.items()}\n",
    "\n",
    "training_data = generate_context_pairs(lines, window_size, vocab)\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = CBOW(len(vocab), embedding_dim).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for context, target in tqdm(training_data, desc=f\"Epoch {epoch+1}\"):\n",
    "        context_tensor = torch.tensor([word2idx[word] for word in context], dtype=torch.long).to(device)\n",
    "        target_tensor = torch.tensor(word2idx[target], dtype=torch.long).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(context_tensor)\n",
    "        loss = loss_fn(outputs, target_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Implement Skipgram:\n",
    "- Create the architecture for Skipgram, which is essentially the inverse of CBOW.\n",
    "- Generate training samples. For each word in the dataset, create pairs with n surrounding words.\n",
    "- Train the model using a suitable optimizer and loss function.\n",
    "- Extract word embeddings for the vocabulary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Evaluation:\n",
    "- Implement a simple cosine similarity function to measure similarity between word pairs.\n",
    "- Test the similarity of a few pairs of words (e.g., king & queen, man & woman, Paris & France).\n",
    "- Visualize embeddings of some selected words using t-SNE or PCA.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Report:\n",
    "- Provide a brief introduction to word embeddings, Skipgram, and CBOW.\n",
    "- Discuss the architecture of the models.\n",
    "- Describe the dataset and pre-processing steps.\n",
    "- Present results from the evaluation step.\n",
    "- Discuss challenges faced during implementation and potential improvements.\n",
    "- Conclude with insights and potential applications of the implemented models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
