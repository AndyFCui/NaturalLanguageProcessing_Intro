{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6120 NLP Fall 2023 Assignment 3\n",
    "\n",
    "## Implementing Skipgram and CBOW Algorithms\n",
    "\n",
    "### Background: \n",
    "Word embeddings are dense vector representations of words in a continuous vector space. Skipgram and CBOW are two primary algorithms introduced by Mikolov et al. in their 2013 papers that form the basis of the popular word2vec model. While both are used for generating word embeddings, they use different architectures and techniques.\n",
    "\n",
    "- Skipgram: Given a word, this model predicts the surrounding context words.\n",
    "- CBOW (Continuous Bag-of-Words): Given context words, this model predicts the target word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks:\n",
    "1. Data Collection:\n",
    "Download the text8 dataset, which is a cleaned version of the first 100MB of the English Wikipedia dump. It is available on several NLP data repositories.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and preparing the text8 dataset\n",
    "import requests\n",
    "\n",
    "# Download the dataset\n",
    "url = \"http://mattmahoney.net/dc/text8.zip\"\n",
    "response = requests.get(url, allow_redirects = True)\n",
    "\n",
    "with open('text8.zip', 'wb') as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Pre-processing:\n",
    "- Tokenize the dataset.\n",
    "- Remove stopwords and non-alphabetic tokens.\n",
    "- Build a vocabulary of the most frequent words (e.g., top 10,000 or 20,000 words).\n",
    "\n",
    "3. Implement CBOW:\n",
    "- Create the architecture for CBOW with an embedding layer and a linear layer.\n",
    "- Generate training samples. For each word in the dataset, use n surrounding words as context.\n",
    "- Train the model using a suitable optimizer and loss function.\n",
    "- Extract word embeddings for the vocabulary.\n",
    "\n",
    "4. Implement Skipgram:\n",
    "- Create the architecture for Skipgram, which is essentially the inverse of CBOW.\n",
    "- Generate training samples. For each word in the dataset, create pairs with n surrounding words.\n",
    "- Train the model using a suitable optimizer and loss function.\n",
    "- Extract word embeddings for the vocabulary.\n",
    "\n",
    "5. Evaluation:\n",
    "- Implement a simple cosine similarity function to measure similarity between word pairs.\n",
    "- Test the similarity of a few pairs of words (e.g., king & queen, man & woman, Paris & France).\n",
    "- Visualize embeddings of some selected words using t-SNE or PCA.\n",
    "\n",
    "6. Report:\n",
    "- Provide a brief introduction to word embeddings, Skipgram, and CBOW.\n",
    "- Discuss the architecture of the models.\n",
    "- Describe the dataset and pre-processing steps.\n",
    "- Present results from the evaluation step.\n",
    "- Discuss challenges faced during implementation and potential improvements.\n",
    "- Conclude with insights and potential applications of the implemented models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import package\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
