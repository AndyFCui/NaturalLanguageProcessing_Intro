{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6120 NLP Fall 2023 Assignment 3\n",
    "\n",
    "## Implementing Skipgram and CBOW Algorithms\n",
    "\n",
    "### Background: \n",
    "Word embeddings are dense vector representations of words in a continuous vector space. Skipgram and CBOW are two primary algorithms introduced by Mikolov et al. in their 2013 papers that form the basis of the popular word2vec model. While both are used for generating word embeddings, they use different architectures and techniques.\n",
    "\n",
    "- Skipgram: Given a word, this model predicts the surrounding context words.\n",
    "- CBOW (Continuous Bag-of-Words): Given context words, this model predicts the target word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks:\n",
    "1. Data Collection:\n",
    "Download the text8 dataset, which is a cleaned version of the first 100MB of the English Wikipedia dump. It is available on several NLP data repositories.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Download the dataset\n",
    "url = \"http://mattmahoney.net/dc/text8.zip\"\n",
    "response = requests.get(url, allow_redirects = True)\n",
    "\n",
    "with open('text8.zip', 'wb') as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philosophy is the belief that rulers are unnecessary and should be abolished although there are differing interpretations of what this means anarchism also refers to related social movements that advocate the elimination of authoritarian institutions particularly the state the word anarchy as most anarchists use it does not imply chaos nihilism or anomie but rather a harmonious anti authoritarian society in place of what are regarded as authoritarian political structures and coercive economic instituti\n",
      "<class 'str'>\n",
      "The length of the `data` is 100000000\n"
     ]
    }
   ],
   "source": [
    "# Loading and preparing the text8 dataset\n",
    "with open('text8', 'r') as file:\n",
    "    data = file.read()\n",
    "    \n",
    "print(data[:1000])\n",
    "print(type(data))\n",
    "print('The length of the {} is {}' .format('`data`', len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Pre-processing:\n",
    "- Tokenize the dataset.\n",
    "- Remove stopwords and non-alphabetic tokens.\n",
    "- Build a vocabulary of the most frequent words (e.g., top 10,000 or 20,000 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import package\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from pprint import pprint\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import  word_tokenize\n",
    "import nltk\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "# add more packages\n",
    "from typing import Dict\n",
    "from typing import Set\n",
    "from typing import List\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from math import sqrt\n",
    "from itertools import chain\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data\n",
    "def clean_corpus(line: list[str]) -> list[str]:\n",
    "    '''\n",
    "    preprocess and clean a given line.\n",
    "\n",
    "    - line: The text line to be cleaned.\n",
    "    ---\n",
    "    - list: A list of preprocessed tokens from the line.\n",
    "    '''\n",
    "    tokens = word_tokenize(line)\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]  \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10888361\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['anarchism',\n",
       " 'originated',\n",
       " 'term',\n",
       " 'abuse',\n",
       " 'first',\n",
       " 'used',\n",
       " 'early',\n",
       " 'working',\n",
       " 'class',\n",
       " 'radicals']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing the data\n",
    "lines: list[list[str]] = []\n",
    "\n",
    "# Predefined list of stop words\n",
    "stop_words: set = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenize the text documents and update the lists word_list and lines\n",
    "lines = clean_corpus(data)\n",
    "print(len(lines))\n",
    "lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one',\n",
       " 'zero',\n",
       " 'nine',\n",
       " 'two',\n",
       " 'eight',\n",
       " 'five',\n",
       " 'three',\n",
       " 'four',\n",
       " 'six',\n",
       " 'seven',\n",
       " 'also',\n",
       " 'first',\n",
       " 'many',\n",
       " 'new',\n",
       " 'used',\n",
       " 'american',\n",
       " 'time',\n",
       " 'see',\n",
       " 'may',\n",
       " 'world']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a vocabulary of the most frequent words (e.g., top 10,000 or 20,000 words).\n",
    "word_counts = Counter(lines)\n",
    "top_freq_words = [word for word, count in word_counts.most_common(10000)]\n",
    "top_freq_words[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsampling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsampling\n",
    "\n",
    "# Flatten the nested list\n",
    "flattened_lines = lines\n",
    "\n",
    "t = 1e-5 # # Hyperparameters\n",
    "\n",
    "word_counts = Counter(flattened_lines)\n",
    "total_count = len(flattened_lines)\n",
    "frequencies = {word: count/total_count for word, count in word_counts.items()}\n",
    "\n",
    "\n",
    "def subsample_prob(word):\n",
    "    prob = max(0, 1 - sqrt(t / frequencies[word]))\n",
    "    return prob\n",
    "\n",
    "subsamped_lines = [word for word in flattened_lines if random.random() > subsample_prob(word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4129394\n"
     ]
    }
   ],
   "source": [
    "print(len(subsamped_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anarchism',\n",
       " 'originated',\n",
       " 'working',\n",
       " 'radicals',\n",
       " 'diggers',\n",
       " 'sans',\n",
       " 'culottes',\n",
       " 'used',\n",
       " 'pejorative',\n",
       " 'organization']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check subsample\n",
    "subsamped_lines[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Implement CBOW:\n",
    "- Create the architecture for CBOW with an embedding layer and a linear layer.\n",
    "- Generate training samples. For each word in the dataset, use n surrounding words as context.\n",
    "- Train the model using a suitable optimizer and loss function.\n",
    "- Extract word embeddings for the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBOW is Continuous Bag Of Words, another vision of Word2Vec\n",
    "- Reference: https://www.youtube.com/watch?v=ghu_5o42QGQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training samples. For each word in the dataset, use n surrounding words as context.\n",
    "def generate_context_pairs(corpus, window_size, vocab_set):\n",
    "    data = []\n",
    "\n",
    "    for i, word in enumerate(corpus):\n",
    "        if word not in vocab_set:\n",
    "            continue\n",
    "\n",
    "        # Initialize an empty context list for the current word\n",
    "        context = []\n",
    "\n",
    "        # Define the start and end indices for the context words\n",
    "        start_index = max(0, i - window_size)\n",
    "        end_index = min(len(corpus), i + window_size + 1)\n",
    "\n",
    "        # Loop over the surrounding words within the window\n",
    "        for j in range(start_index, end_index):\n",
    "            # Exclude the current word itself\n",
    "            if j != i and corpus[j] in vocab_set:\n",
    "                context.append(corpus[j])\n",
    "\n",
    "        target = word\n",
    "        data.append((context, target))\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair 1:\n",
      "Context: ['originated', 'working']\n",
      "Target: anarchism\n",
      "----------------------\n",
      "Pair 2:\n",
      "Context: ['anarchism', 'working']\n",
      "Target: originated\n",
      "----------------------\n",
      "Pair 3:\n",
      "Context: ['anarchism', 'originated']\n",
      "Target: working\n",
      "----------------------\n",
      "Pair 4:\n",
      "Context: ['organization']\n",
      "Target: used\n",
      "----------------------\n",
      "Pair 5:\n",
      "Context: ['used']\n",
      "Target: organization\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "# check function of genreate\n",
    "context_pairs = generate_context_pairs(subsamped_lines[:10], 2, top_freq_words)\n",
    "\n",
    "# print head 10\n",
    "for i, (context, target) in enumerate(context_pairs[:10]):\n",
    "    print(f\"Pair {i+1}:\")\n",
    "    print(\"Context:\", context)\n",
    "    print(\"Target:\", target)\n",
    "    print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the architecture for CBOW with an embedding layer and a linear layer.\n",
    "# Use DataLoader\n",
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, training_data, word2idx):\n",
    "        self.training_data = training_data\n",
    "        self.word2idx = word2idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.training_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.training_data[idx]\n",
    "        context_tensor = torch.tensor([self.word2idx[word] for word in context], dtype=torch.long)\n",
    "        target_tensor = torch.tensor(self.word2idx[target], dtype=torch.long)\n",
    "        return context_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New DataLoader instance\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "train_dataset = CBOWDataset(training_data, word2idx)\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size = BATCH_SIZE, \n",
    "                          shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# check GPU is work\n",
    "# Train the model using a suitable optimizer and loss function.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 11233/2413757 [00:15<55:41, 719.08it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\GithubRepo\\NaturalLanguageProcessing_Intro\\as3\\CS6120_NLP_Assignment_3.ipynb Cell 20\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GithubRepo/NaturalLanguageProcessing_Intro/as3/CS6120_NLP_Assignment_3.ipynb#X20sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(outputs, target_tensor)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GithubRepo/NaturalLanguageProcessing_Intro/as3/CS6120_NLP_Assignment_3.ipynb#X20sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/GithubRepo/NaturalLanguageProcessing_Intro/as3/CS6120_NLP_Assignment_3.ipynb#X20sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GithubRepo/NaturalLanguageProcessing_Intro/as3/CS6120_NLP_Assignment_3.ipynb#X20sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GithubRepo/NaturalLanguageProcessing_Intro/as3/CS6120_NLP_Assignment_3.ipynb#X20sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m epoch_losses\u001b[39m.\u001b[39mappend(total_loss)\n",
      "File \u001b[1;32mc:\\Users\\Andy Cui\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\optim\\optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             )\n\u001b[1;32m--> 373\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    374\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    376\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Andy Cui\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\Andy Cui\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\optim\\sgd.py:75\u001b[0m, in \u001b[0;36mSGD.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     71\u001b[0m momentum_buffer_list \u001b[39m=\u001b[39m []\n\u001b[0;32m     73\u001b[0m has_sparse_grad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(group, params_with_grad, d_p_list, momentum_buffer_list)\n\u001b[1;32m---> 75\u001b[0m sgd(params_with_grad,\n\u001b[0;32m     76\u001b[0m     d_p_list,\n\u001b[0;32m     77\u001b[0m     momentum_buffer_list,\n\u001b[0;32m     78\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     79\u001b[0m     momentum\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmomentum\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     80\u001b[0m     lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     81\u001b[0m     dampening\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdampening\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     82\u001b[0m     nesterov\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mnesterov\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     83\u001b[0m     maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     84\u001b[0m     has_sparse_grad\u001b[39m=\u001b[39;49mhas_sparse_grad,\n\u001b[0;32m     85\u001b[0m     foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     87\u001b[0m \u001b[39m# update momentum_buffers in state\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[39mfor\u001b[39;00m p, momentum_buffer \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(params_with_grad, momentum_buffer_list):\n",
      "File \u001b[1;32mc:\\Users\\Andy Cui\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\optim\\sgd.py:220\u001b[0m, in \u001b[0;36msgd\u001b[1;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    218\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_sgd\n\u001b[1;32m--> 220\u001b[0m func(params,\n\u001b[0;32m    221\u001b[0m      d_p_list,\n\u001b[0;32m    222\u001b[0m      momentum_buffer_list,\n\u001b[0;32m    223\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    224\u001b[0m      momentum\u001b[39m=\u001b[39;49mmomentum,\n\u001b[0;32m    225\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    226\u001b[0m      dampening\u001b[39m=\u001b[39;49mdampening,\n\u001b[0;32m    227\u001b[0m      nesterov\u001b[39m=\u001b[39;49mnesterov,\n\u001b[0;32m    228\u001b[0m      has_sparse_grad\u001b[39m=\u001b[39;49mhas_sparse_grad,\n\u001b[0;32m    229\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize)\n",
      "File \u001b[1;32mc:\\Users\\Andy Cui\\.conda\\envs\\nlp\\lib\\site-packages\\torch\\optim\\sgd.py:327\u001b[0m, in \u001b[0;36m_multi_tensor_sgd\u001b[1;34m(params, grads, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001b[0m\n\u001b[0;32m    324\u001b[0m         device_grads \u001b[39m=\u001b[39m bufs\n\u001b[0;32m    326\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m device_has_sparse_grad:\n\u001b[1;32m--> 327\u001b[0m     torch\u001b[39m.\u001b[39;49m_foreach_add_(device_params, device_grads, alpha\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49mlr)\n\u001b[0;32m    328\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    329\u001b[0m     \u001b[39m# foreach APIs don't support sparse\u001b[39;00m\n\u001b[0;32m    330\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(device_params)):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 20\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "window_size = 2\n",
    "\n",
    "# Preparing data\n",
    "vocab = set(top_freq_words)\n",
    "word2idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx2word = {i: word for word, i in word2idx.items()}\n",
    "\n",
    "training_data = generate_context_pairs(subsamped_lines, window_size, vocab)\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = CBOW(len(vocab), embedding_dim).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# List to store losses\n",
    "epoch_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for context, target in tqdm(training_data, desc=f\"Epoch {epoch+1}\"):\n",
    "        context_tensor = torch.tensor([word2idx[word] \n",
    "                                       for word in context], \n",
    "                                      dtype = torch.long).to(device)\n",
    "        \n",
    "        target_tensor = torch.tensor(word2idx[target], \n",
    "                                     dtype = torch.long).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(context_tensor)\n",
    "        loss = loss_fn(outputs, target_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    epoch_losses.append(total_loss)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Implement Skipgram:\n",
    "- Create the architecture for Skipgram, which is essentially the inverse of CBOW.\n",
    "- Generate training samples. For each word in the dataset, create pairs with n surrounding words.\n",
    "- Train the model using a suitable optimizer and loss function.\n",
    "- Extract word embeddings for the vocabulary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Evaluation:\n",
    "- Implement a simple cosine similarity function to measure similarity between word pairs.\n",
    "- Test the similarity of a few pairs of words (e.g., king & queen, man & woman, Paris & France).\n",
    "- Visualize embeddings of some selected words using t-SNE or PCA.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Report:\n",
    "- Provide a brief introduction to word embeddings, Skipgram, and CBOW.\n",
    "- Discuss the architecture of the models.\n",
    "- Describe the dataset and pre-processing steps.\n",
    "- Present results from the evaluation step.\n",
    "- Discuss challenges faced during implementation and potential improvements.\n",
    "- Conclude with insights and potential applications of the implemented models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
