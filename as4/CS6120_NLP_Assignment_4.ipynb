{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6120 NLP Fall 2023 Assignment 4\n",
    "\n",
    "## About the transformer\n",
    "\n",
    "### 1. Background: \n",
    "- Study the transformer code provided\n",
    "- Apply Transformer code provided in the module to train a language models that generates financial. discourse in Warren Buffet's style. Train you model using Warren Buffets's Annual Letters to shareholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1. Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU.\n"
     ]
    }
   ],
   "source": [
    "# check GPU is work\n",
    "# Train the model using a suitable optimizer and loss function.\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "''' Look at all previous tokens to generate next\n",
    "    @Author: Uzair Ahmad\n",
    "    2022\n",
    "    +TransformerBlock \n",
    "'''\n",
    "\n",
    "\n",
    "class TransformerBlockLM(nn.Module):\n",
    "    class TransformerBlock(nn.Module):\n",
    "        def __init__(self, head_count, in_size, out_size):\n",
    "            super().__init__()\n",
    "            self.comm = TransformerBlockLM.MultiHeadAttention(head_count=head_count,\n",
    "                                                              in_size=in_size,\n",
    "                                                              out_size=out_size)\n",
    "            self.think = TransformerBlockLM.MLP(embed_size=out_size)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return x + self.think(x + self.comm(x))\n",
    "\n",
    "    class MLP(nn.Module):\n",
    "        # FFNN (embed_size, embed_size*4, embed_size)\n",
    "        def __init__(self, embed_size):\n",
    "            super().__init__()\n",
    "            self.mlp = nn.Sequential(nn.Linear(embed_size, embed_size * 4),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(embed_size * 4, embed_size))\n",
    "            self.layerNorm = nn.LayerNorm(embed_size)\n",
    "\n",
    "        def forward(self, x):  # think\n",
    "            return self.layerNorm(self.mlp(x))  # paper - after\n",
    "            # return self.mlp(self.layerNorm(x)) # alternate - before\n",
    "\n",
    "    class MultiHeadAttention(nn.Module):\n",
    "        \"\"\"\n",
    "        multiple parallel SA heads (communication among words)\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, head_count, in_size, out_size):\n",
    "            super().__init__()\n",
    "            self.heads = nn.ModuleList(\n",
    "                TransformerBlockLM.SelfAttentionHead(in_size, out_size // head_count)\n",
    "                for _ in range(head_count)\n",
    "            )\n",
    "            self.layerNorm = nn.LayerNorm(out_size)\n",
    "            # self.proj = nn.Linear(out_size, out_size)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # concat over channel/embeddings_size dimension\n",
    "            return self.layerNorm(torch.cat([head(x) for head in self.heads], dim=-1))  # paper - after\n",
    "            # return torch.cat([head(self.layerNorm(x)) for head in self.heads], dim=-1) # alternate - before\n",
    "            # return self.proj(torch.cat([head(x) for head in self.heads], dim=-1))\n",
    "\n",
    "    class SelfAttentionHead(nn.Module):\n",
    "        def __init__(self, in_size, out_size):\n",
    "            \"\"\"\n",
    "            in_size is embed_size\n",
    "            out_size is head_size\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            self.head_size = out_size\n",
    "            self.K = nn.Linear(in_size, self.head_size, bias=False)\n",
    "            self.Q = nn.Linear(in_size, self.head_size, bias=False)\n",
    "            self.V = nn.Linear(in_size, self.head_size, bias=False)\n",
    "\n",
    "        def forward(self, x):\n",
    "            keys = self.K(x)\n",
    "            queries = self.Q(x)\n",
    "            # affinities :\n",
    "            # all the queries will dot-product with all the keys\n",
    "            # transpose (swap) second dimension (input_length) with third (head_size)\n",
    "            keys_t = keys.transpose(1, 2)\n",
    "            autocorrs = (queries @ keys_t) * (self.head_size ** -0.5)  # (batch_size x input_length x input_length)\n",
    "            '''\n",
    "            (batch_size x input_length x embed_size) @ (batch_size x embed_size x input_length) ----> (batch_size x input_length x input_length)\n",
    "            '''\n",
    "            autocorrs = torch.tril(autocorrs)\n",
    "            autocorrs = autocorrs.masked_fill(autocorrs == 0, float('-inf'))\n",
    "            autocorrs = torch.softmax(autocorrs, dim=-1)\n",
    "            values = self.V(x)  # (batch_size x input_length x head_size)\n",
    "            out = autocorrs @ values\n",
    "            return out\n",
    "\n",
    "    def __init__(self, batch_size=4,\n",
    "                 input_length=8,\n",
    "                 embed_size=16,\n",
    "                 sa_head_size=8,\n",
    "                 sa_multihead_count=4,\n",
    "                 pos_embed=False,\n",
    "                 include_mlp=False):\n",
    "        super().__init__()\n",
    "        self.blocks = None\n",
    "        self.ffn = None\n",
    "        self.sa_heads = None\n",
    "        # sa_head_size head_size of self-attention module\n",
    "        self.sa_head_size = sa_head_size\n",
    "        self.sa_multihead_count = sa_multihead_count\n",
    "\n",
    "        self.val_data = None\n",
    "        self.train_data = None\n",
    "        self.val_text = None\n",
    "        self.train_text = None\n",
    "        self.K = None\n",
    "        self.linear_sahead_to_vocab = None\n",
    "        self.vocab = None\n",
    "        self.token_embeddings_table = None\n",
    "        self.vocab_size = None\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.vocab_size: int\n",
    "        self.is_pos_emb = pos_embed\n",
    "        self.include_mlp = include_mlp\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        # input_length = how many consecutive tokens/chars in one input\n",
    "        self.input_length = input_length\n",
    "        # batch_size = how many inputs are going to be processed in-parallel (on GPU)\n",
    "        self.batch_size = batch_size\n",
    "        # embed_size = embedding size\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.lm_head = None\n",
    "        self.position_embeddings_table = None\n",
    "\n",
    "    def forward(self, in_ids, target=None):\n",
    "        in_ids_emb = self.token_embeddings_table(in_ids[:, -self.input_length:])\n",
    "        if self.is_pos_emb:\n",
    "            in_ids_pos_emb = self.position_embeddings_table(\n",
    "                torch.arange(in_ids[:, -self.input_length:].shape[1], device=self.device)\n",
    "            )\n",
    "            in_ids_emb = in_ids_emb + in_ids_pos_emb\n",
    "\n",
    "        block_outputs = self.blocks(in_ids_emb)\n",
    "        logits = self.linear_sahead_to_vocab(block_outputs)  # compute\n",
    "\n",
    "        if target is None:\n",
    "            ce_loss = None\n",
    "        else:\n",
    "            batch_size, input_length, vocab_size = logits.shape\n",
    "            logits_ = logits.view(batch_size * input_length, vocab_size)\n",
    "            targets = target.view(batch_size * input_length)\n",
    "            ce_loss = F.cross_entropy(logits_, targets)\n",
    "        return logits, ce_loss\n",
    "\n",
    "    def fit(self, train_iters=100, eval_iters=10, lr=0.0001):\n",
    "        \"\"\"\n",
    "        train_iters = how many training iterations\n",
    "        eval_iters = how many batches to evaluate to get average performance\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        for iteration in range(train_iters):\n",
    "            if iteration % eval_iters == 0:\n",
    "                avg_loss = self.eval_loss(eval_iters)\n",
    "                print(f\"iter {iteration}: train {avg_loss['train']} val {avg_loss['eval']}\")\n",
    "            inputs, targets = self.get_batch(split='train')\n",
    "            _, ce_loss = self(inputs, targets)\n",
    "            optimizer.zero_grad(set_to_none=True)  # clear gradients of previous step\n",
    "            ce_loss.backward()  # propagate loss back to each unit in the network\n",
    "            optimizer.step()  # update network parameters w.r.t the loss\n",
    "        # torch.save(self, 'sa_pos_')\n",
    "\n",
    "    def generate(self, context_token_ids, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            token_rep, _ = self(context_token_ids)\n",
    "            last_token_rep = token_rep[:, -1, :]\n",
    "            probs = F.softmax(last_token_rep, dim=1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            context_token_ids = torch.cat((context_token_ids, next_token), dim=1)\n",
    "        output_text = self.decoder(context_token_ids[0].tolist())\n",
    "        return output_text\n",
    "\n",
    "    @torch.no_grad()  # tell torch not to prepare for back-propagation (context manager)\n",
    "    def eval_loss(self, eval_iters):\n",
    "        perf = {}\n",
    "        # set dropout and batch normalization layers to evaluation mode before running inference.\n",
    "        self.eval()\n",
    "        for split in ['train', 'eval']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                tokens, targets = self.get_batch(split)  # get random batch of inputs and targete\n",
    "                _, ce_loss = self(tokens, targets)  # forward pass\n",
    "                losses[k] = ce_loss.item()  # the value of loss tensor as a standard Python number\n",
    "            perf[split] = losses.mean()\n",
    "        self.train()  # turn-on training mode-\n",
    "        return perf\n",
    "\n",
    "    def prep(self, corpus):\n",
    "        self.vocab = sorted(list(set(corpus)))\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        c2i = {c: i for i, c in\n",
    "               enumerate(self.vocab)}  # char c to integer i map. assign value i for every word in vocab\n",
    "        i2c = {i: c for c, i in c2i.items()}  # integer i to char c map\n",
    "\n",
    "        self.encoder = lambda doc: [c2i[c] for c in doc]\n",
    "        self.decoder = lambda nums: ''.join([i2c[i] for i in nums])\n",
    "\n",
    "        n = len(text)\n",
    "        self.train_text = text[:int(n * 0.9)]\n",
    "        self.val_text = text[int(n * 0.9):]\n",
    "\n",
    "        self.train_data = torch.tensor(self.encoder(self.train_text), \n",
    "                                       dtype=torch.long, \n",
    "                                       device=self.device)\n",
    "        self.val_data = torch.tensor(self.encoder(self.val_text), \n",
    "                                     dtype=torch.long,\n",
    "                                     device=self.device)\n",
    "\n",
    "        # look-up table for embeddings (vocab_size x embed_size)\n",
    "        # it will be mapping each token id to a vector of embed_size\n",
    "        # a wrapper to store vector representations of each token\n",
    "        self.token_embeddings_table = \\\n",
    "            nn.Embedding(self.vocab_size, self.embed_size).to(self.device)\n",
    "\n",
    "        if self.is_pos_emb:\n",
    "            self.position_embeddings_table = nn.Embedding(self.input_length, self.embed_size).to(self.device)\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "        )\n",
    "        # linear projection of sa_head output to vocabulary\n",
    "        self.linear_sahead_to_vocab = nn.Linear(self.sa_head_size, self.vocab_size)\n",
    "\n",
    "    def get_batch(self, split='train'):\n",
    "        data = self.train_data if split == 'train' else self.val_data\n",
    "        # get random chunks of length batch_size from data\n",
    "        ix = torch.randint(len(data) - self.input_length,\n",
    "                           (self.batch_size,))\n",
    "        inputs_batch = torch.stack([data[i:i + self.input_length] for i in ix])\n",
    "        targets_batch = torch.stack([data[i + 1:i + self.input_length + 1] for i in ix])\n",
    "        inputs_batch = inputs_batch.to(self.device)\n",
    "        targets_batch = targets_batch.to(self.device)\n",
    "        # inputs_batch is\n",
    "        return inputs_batch, targets_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params 1112398\n",
      "iter 0: train 5.391458034515381 val 5.360659122467041\n",
      "iter 1000: train 1.647735357284546 val 1.708343744277954\n",
      "iter 2000: train 1.369718313217163 val 1.7124419212341309\n",
      "iter 3000: train 1.1540275812149048 val 1.8630141019821167\n",
      "\n",
      "As if I butch the fall is the grave\n",
      "   His superfluous all futures travelled both,\n",
      "It assasin familiar guest;\n",
      "Hint, your little, I would countenance\n",
      "As golden break for me\n",
      "All other passed her bacinant\n",
      "   Him goneaden bure.\n",
      "\n",
      "XI.\n",
      "\n",
      "I much joy can from somer\n",
      "That skirror\n",
      "   I have cunetess music, sometimes of gold\n",
      "On bleading to break,\n",
      "Reces if I fermanted daffodil\n",
      "For himself it hinder to fit;\n",
      "There imperial its propice\n",
      "(t any passed\n",
      "   Behind a single clause,\n",
      " Fuspirite fores on my stem,\n",
      "\" and name my more you!\n",
      "I mind my time, apart hope, for see\n",
      "   To have the summer eymed homesty fingers with its passed\n",
      "   They will of finished sinks,\n",
      "Till quaint that way.\n",
      "To Im s there, —\n",
      "Him stares in given lip,\n",
      "   Poroporog absort trem\n",
      "And siervel as yet done, —\n",
      "Spendant her girl, — and alone!\n",
      "\n",
      " \n",
      "V.\n",
      "\n",
      "Unmoved, an emperor is over!\n",
      "\n",
      "For thee are heaven, — but the bog?\n",
      "Past summon the goal,\n",
      "I will, and the air trost;\n",
      "His softer swransition\n",
      "   Nature remain;\n",
      "It is not bog the sky\n",
      "         They'd jest\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "with open('./emily_dickonson.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# text = 'a quick brown fox jumps over the lazy dog.\\n ' \\\n",
    "#        'lazy dog and a quick brown fox.\\n' \\\n",
    "#        'the dog is lazy and the fox jumps quickly.\\n' \\\n",
    "#        'a fox jumps over the dog because he is lazy.\\n' \\\n",
    "#        'dog is lazy and fox is brown. she quickly jumps over the lazy dog.'\n",
    "\n",
    "model = TransformerBlockLM(batch_size=64,\n",
    "                           input_length=32,\n",
    "                           embed_size=128,\n",
    "                           sa_multihead_count=8,\n",
    "                           sa_head_size=128,\n",
    "                           pos_embed=True,\n",
    "                           include_mlp=True)\n",
    "model.prep(text)\n",
    "model = model.to(model.device)\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "print(f'params {sum([np.prod(p.size()) for p in model_parameters])}')\n",
    "input_batch, output_batch = model.get_batch(split='train')\n",
    "_, _ = model(input_batch, output_batch)\n",
    "model.fit(train_iters=4000, eval_iters=1000, lr=1e-3)\n",
    "outputs = model.generate(context_token_ids=torch.zeros((1, 1),\n",
    "                                                        dtype=torch.long,\n",
    "                                                        device=model.device),\n",
    "                         max_new_tokens=1000)\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Identify the points where code is different from the proposed architecture in Google's patent (provided in the module) (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main different from the google's patent with our `LM_TransformerBlock_MLP_PosEmb_AddNormOpt.py` is that:\n",
    "-  Encoder-decoder architecture: Google's patent describes a system with clearly separated encoders and decoders. While our TransformerBlockLM code implements a Transformer module, the Python code implements a Transformer module, which can be part of the encoder or decoder, but does not clearly distinguish between the two components.\n",
    "\n",
    "- Self-attention and feed-forward layers: The patent may describe a more general architecture, but the code we used implements a specific self-attention and feed-forward layer structure, and may include the layer normalization step, which There may not be a clear description in the patent.\n",
    "\n",
    "- Autoregressive generation: The decoder in Google’s patent appears to use an autoregressive approach to generate the output sequence. We have a similar implementation in our code that implements an autoregressive generating function, but it's not clear how this aligns with the specific mechanism described in the patent.\n",
    "\n",
    "- Output generation: The decoder mentioned in the patent may use linear layers and softmax layers to generate output probability distributions. Although our code may take a similar approach, the specific configuration and parameters may be different.\n",
    "\n",
    "- Training and evaluation: Our code contains specific training and evaluation methods that may not be detailed in Google patents. Our python implementation may include specific details such as loss function calculation, gradient descent, and performance evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Re-design the code to make it more intuitive. Give arguments why do you think your code is better. (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I. Fix bugs\n",
    "\n",
    "- change two device argument to on like gpu not cpu. I switch the line 17 and 18. \n",
    "- model.prep(text)\n",
    "- model = model.to(model.device)\n",
    "\n",
    "Then we can fix them use gpu. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II. Add plot to show the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I add a function draw the plot to show the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III. Re-design for `fit(self, train_iters=100, eval_iters=10, lr=0.0001)`\n",
    "- I add avarable paramter about `train_losses` , `val__losses` and `iterations`. Then I can draw the plot base on these paramter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Apply Transformer code provided in the module to train a language models that generates financial discourse in Warren Buffet's style. Train you model using Warren Buffets's Annual Letters to shareholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERKSHIRE HATHAWAY INC. \n",
      "\n",
      "\n",
      "\n",
      "To the Shareholders of Berkshire Hathaway Inc.: \n",
      "\n",
      "Our gain in net worth during 2006 was $16.9 billion, which increased the per-share book value of \n",
      "both our Class A and Class B stock by 18.4%. Over the last 42 years (that is, since present management \n",
      "took over) book value has grown from $19 to $70,281, a rate of 21.4% compounded annually.* \n",
      "\n",
      "We believe that $16.9 billion is a record for a one-year gain in net worth - more than has ever \n",
      "been booked by any American business, leaving aside boosts that have occurred because of mergers (e.g., \n",
      "AOL's purchase of Time Warner). Of course, Exxon Mobil and other companies earn far more than \n",
      "Berkshire, but their earnings largely go to dividends and/or repurchases, rather than to building net worth. \n",
      "\n",
      "All that said, a confession about our 2006 gain is in order. Our most important business, \n",
      "insurance, benefited from a large dose of luck: Mother Nature, bless her heart, went on vacation. After \n",
      "hammering us with hurri\n",
      "<class 'str'>\n",
      "The length of the `data` is 330140\n"
     ]
    }
   ],
   "source": [
    "# Loading and preparing the Warren Buffet dataset\n",
    "with open('WarrenBuffet.txt', 'r') as file:\n",
    "    data = file.read()\n",
    "    \n",
    "print(data[:1000])\n",
    "print(type(data))\n",
    "print('The length of the {} is {}' .format('`data`', len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import package\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pre-processing:\n",
    "- Tokenize the dataset.\n",
    "- Remove stopwords and non-alphabetic tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data\n",
    "def clean_corpus(line: list[str]) -> list[str]:\n",
    "    '''\n",
    "    preprocess and clean a given line.\n",
    "\n",
    "    - line: The text line to be cleaned.\n",
    "    ---\n",
    "    - list: A list of preprocessed tokens from the line.\n",
    "    '''\n",
    "    tokens = word_tokenize(line.lower())\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]  \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26557\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['berkshire',\n",
       " 'hathaway',\n",
       " 'shareholders',\n",
       " 'berkshire',\n",
       " 'hathaway',\n",
       " 'gain',\n",
       " 'net',\n",
       " 'worth',\n",
       " 'billion',\n",
       " 'increased']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing the data\n",
    "lines: list[list[str]] = []\n",
    "\n",
    "# Predefined list of stop words\n",
    "stop_words: set = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenize the text documents and update the lists word_list and lines\n",
    "lines = clean_corpus(data)\n",
    "print(len(lines))\n",
    "lines[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement TransformerBlockLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlockLM(nn.Module):\n",
    "    class TransformerBlock(nn.Module):\n",
    "        def __init__(self, head_count, in_size, out_size):\n",
    "            super().__init__()\n",
    "            self.comm = TransformerBlockLM.MultiHeadAttention(head_count=head_count,\n",
    "                                                              in_size=in_size,\n",
    "                                                              out_size=out_size)\n",
    "            self.think = TransformerBlockLM.MLP(embed_size=out_size)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return x + self.think(x + self.comm(x))\n",
    "\n",
    "    class MLP(nn.Module):\n",
    "        # FFNN (embed_size, embed_size*4, embed_size)\n",
    "        def __init__(self, embed_size):\n",
    "            super().__init__()\n",
    "            self.mlp = nn.Sequential(nn.Linear(embed_size, embed_size * 4),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(embed_size * 4, embed_size))\n",
    "            self.layerNorm = nn.LayerNorm(embed_size)\n",
    "\n",
    "        def forward(self, x):  # think\n",
    "            return self.layerNorm(self.mlp(x))  # paper - after\n",
    "            # return self.mlp(self.layerNorm(x)) # alternate - before\n",
    "\n",
    "    class MultiHeadAttention(nn.Module):\n",
    "        \"\"\"\n",
    "        multiple parallel SA heads (communication among words)\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, head_count, in_size, out_size):\n",
    "            super().__init__()\n",
    "            self.heads = nn.ModuleList(\n",
    "                TransformerBlockLM.SelfAttentionHead(in_size, out_size // head_count)\n",
    "                for _ in range(head_count)\n",
    "            )\n",
    "            self.layerNorm = nn.LayerNorm(out_size)\n",
    "            # self.proj = nn.Linear(out_size, out_size)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # concat over channel/embeddings_size dimension\n",
    "            return self.layerNorm(torch.cat([head(x) for head in self.heads], dim=-1))  # paper - after\n",
    "            # return torch.cat([head(self.layerNorm(x)) for head in self.heads], dim=-1) # alternate - before\n",
    "            # return self.proj(torch.cat([head(x) for head in self.heads], dim=-1))\n",
    "\n",
    "    class SelfAttentionHead(nn.Module):\n",
    "        def __init__(self, in_size, out_size):\n",
    "            \"\"\"\n",
    "            in_size is embed_size\n",
    "            out_size is head_size\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            self.head_size = out_size\n",
    "            self.K = nn.Linear(in_size, self.head_size, bias=False)\n",
    "            self.Q = nn.Linear(in_size, self.head_size, bias=False)\n",
    "            self.V = nn.Linear(in_size, self.head_size, bias=False)\n",
    "\n",
    "        def forward(self, x):\n",
    "            keys = self.K(x)\n",
    "            queries = self.Q(x)\n",
    "            # affinities :\n",
    "            # all the queries will dot-product with all the keys\n",
    "            # transpose (swap) second dimension (input_length) with third (head_size)\n",
    "            keys_t = keys.transpose(1, 2)\n",
    "            autocorrs = (queries @ keys_t) * (self.head_size ** -0.5)  # (batch_size x input_length x input_length)\n",
    "            '''\n",
    "            (batch_size x input_length x embed_size) @ (batch_size x embed_size x input_length) ----> (batch_size x input_length x input_length)\n",
    "            '''\n",
    "            autocorrs = torch.tril(autocorrs)\n",
    "            autocorrs = autocorrs.masked_fill(autocorrs == 0, float('-inf'))\n",
    "            autocorrs = torch.softmax(autocorrs, dim=-1)\n",
    "            values = self.V(x)  # (batch_size x input_length x head_size)\n",
    "            out = autocorrs @ values\n",
    "            return out\n",
    "\n",
    "    def __init__(self, batch_size=4,\n",
    "                 input_length=8,\n",
    "                 embed_size=16,\n",
    "                 sa_head_size=8,\n",
    "                 sa_multihead_count=4,\n",
    "                 pos_embed=False,\n",
    "                 include_mlp=False):\n",
    "        super().__init__()\n",
    "        self.blocks = None\n",
    "        self.ffn = None\n",
    "        self.sa_heads = None\n",
    "        # sa_head_size head_size of self-attention module\n",
    "        self.sa_head_size = sa_head_size\n",
    "        self.sa_multihead_count = sa_multihead_count\n",
    "\n",
    "        self.val_data = None\n",
    "        self.train_data = None\n",
    "        self.val_text = None\n",
    "        self.train_text = None\n",
    "        self.K = None\n",
    "        self.linear_sahead_to_vocab = None\n",
    "        self.vocab = None\n",
    "        self.token_embeddings_table = None\n",
    "        self.vocab_size = None\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.vocab_size: int\n",
    "        self.is_pos_emb = pos_embed\n",
    "        self.include_mlp = include_mlp\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        # input_length = how many consecutive tokens/chars in one input\n",
    "        self.input_length = input_length\n",
    "        # batch_size = how many inputs are going to be processed in-parallel (on GPU)\n",
    "        self.batch_size = batch_size\n",
    "        # embed_size = embedding size\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.lm_head = None\n",
    "        self.position_embeddings_table = None\n",
    "\n",
    "    def forward(self, in_ids, target=None):\n",
    "        in_ids_emb = self.token_embeddings_table(in_ids[:, -self.input_length:])\n",
    "        if self.is_pos_emb:\n",
    "            in_ids_pos_emb = self.position_embeddings_table(\n",
    "                torch.arange(in_ids[:, -self.input_length:].shape[1], device=self.device)\n",
    "            )\n",
    "            in_ids_emb = in_ids_emb + in_ids_pos_emb\n",
    "\n",
    "        block_outputs = self.blocks(in_ids_emb)\n",
    "        logits = self.linear_sahead_to_vocab(block_outputs)  # compute\n",
    "\n",
    "        if target is None:\n",
    "            ce_loss = None\n",
    "        else:\n",
    "            batch_size, input_length, vocab_size = logits.shape\n",
    "            logits_ = logits.view(batch_size * input_length, vocab_size)\n",
    "            targets = target.view(batch_size * input_length)\n",
    "            ce_loss = F.cross_entropy(logits_, targets)\n",
    "        return logits, ce_loss\n",
    "\n",
    "    def fit(self, train_iters=100, eval_iters=10, lr=0.0001):\n",
    "        \"\"\"\n",
    "        train_iters = how many training iterations\n",
    "        eval_iters = how many batches to evaluate to get average performance\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        \n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        iterations = []\n",
    "        \n",
    "        for iteration in range(train_iters):\n",
    "            if iteration % eval_iters == 0:\n",
    "                avg_loss = self.eval_loss(eval_iters)\n",
    "                print(f\"iter {iteration}: train {avg_loss['train']} val {avg_loss['eval']}\")\n",
    "                iterations.append(iteration)\n",
    "                train_losses.append(avg_loss['train'].item())  # .item() to get the Python number from tensor\n",
    "                val_losses.append(avg_loss['eval'].item())\n",
    "            inputs, targets = self.get_batch(split='train')\n",
    "            _, ce_loss = self(inputs, targets)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            ce_loss.backward()\n",
    "            optimizer.step()\n",
    "        # Return the collected data\n",
    "        return iterations, train_losses, val_losses  \n",
    "\n",
    "    def generate(self, context_token_ids, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            token_rep, _ = self(context_token_ids)\n",
    "            last_token_rep = token_rep[:, -1, :]\n",
    "            probs = F.softmax(last_token_rep, dim=1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            context_token_ids = torch.cat((context_token_ids, next_token), dim=1)\n",
    "        output_text = self.decoder(context_token_ids[0].tolist())\n",
    "        return output_text\n",
    "\n",
    "    @torch.no_grad()  # tell torch not to prepare for back-propagation (context manager)\n",
    "    def eval_loss(self, eval_iters):\n",
    "        perf = {}\n",
    "        # set dropout and batch normalization layers to evaluation mode before running inference.\n",
    "        self.eval()\n",
    "        for split in ['train', 'eval']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                tokens, targets = self.get_batch(split)  # get random batch of inputs and targete\n",
    "                _, ce_loss = self(tokens, targets)  # forward pass\n",
    "                losses[k] = ce_loss.item()  # the value of loss tensor as a standard Python number\n",
    "            perf[split] = losses.mean()\n",
    "        self.train()  # turn-on training mode-\n",
    "        return perf\n",
    "\n",
    "    def prep(self, corpus):\n",
    "        self.vocab = sorted(list(set(corpus)))\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        c2i = {c: i for i, c in\n",
    "               enumerate(self.vocab)}  # char c to integer i map. assign value i for every word in vocab\n",
    "        i2c = {i: c for c, i in c2i.items()}  # integer i to char c map\n",
    "\n",
    "        self.encoder = lambda doc: [c2i[c] for c in doc]\n",
    "        self.decoder = lambda nums: ''.join([i2c[i] for i in nums])\n",
    "\n",
    "        n = len(text)\n",
    "        self.train_text = text[:int(n * 0.9)]\n",
    "        self.val_text = text[int(n * 0.9):]\n",
    "\n",
    "        self.train_data = torch.tensor(self.encoder(self.train_text), \n",
    "                                       dtype=torch.long, \n",
    "                                       device=self.device)\n",
    "        self.val_data = torch.tensor(self.encoder(self.val_text), \n",
    "                                     dtype=torch.long,\n",
    "                                     device=self.device)\n",
    "\n",
    "        # look-up table for embeddings (vocab_size x embed_size)\n",
    "        # it will be mapping each token id to a vector of embed_size\n",
    "        # a wrapper to store vector representations of each token\n",
    "        self.token_embeddings_table = \\\n",
    "            nn.Embedding(self.vocab_size, self.embed_size).to(self.device)\n",
    "\n",
    "        if self.is_pos_emb:\n",
    "            self.position_embeddings_table = nn.Embedding(self.input_length, self.embed_size).to(self.device)\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "        )\n",
    "        # linear projection of sa_head output to vocabulary\n",
    "        self.linear_sahead_to_vocab = nn.Linear(self.sa_head_size, self.vocab_size)\n",
    "\n",
    "    def get_batch(self, split='train'):\n",
    "        data = self.train_data if split == 'train' else self.val_data\n",
    "        # get random chunks of length batch_size from data\n",
    "        ix = torch.randint(len(data) - self.input_length,\n",
    "                           (self.batch_size,))\n",
    "        inputs_batch = torch.stack([data[i:i + self.input_length] for i in ix])\n",
    "        targets_batch = torch.stack([data[i + 1:i + self.input_length + 1] for i in ix])\n",
    "        inputs_batch = inputs_batch.to(self.device)\n",
    "        targets_batch = targets_batch.to(self.device)\n",
    "        # inputs_batch is\n",
    "        return inputs_batch, targets_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerBlockLM(batch_size=64,\n",
    "                           input_length=32,\n",
    "                           embed_size=128,\n",
    "                           sa_multihead_count=8,\n",
    "                           sa_head_size=128,\n",
    "                           pos_embed=True,\n",
    "                           include_mlp=True)\n",
    "model.prep(text)\n",
    "model = model.to(model.device)\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "print(f'params {sum([np.prod(p.size()) for p in model_parameters])}')\n",
    "input_batch, output_batch = model.get_batch(split='train')\n",
    "_, _ = model(input_batch, output_batch)\n",
    "model.fit(train_iters=4000, eval_iters=1000, lr=1e-3)\n",
    "outputs = model.generate(context_token_ids=torch.zeros((1, 1),\n",
    "                                                        dtype=torch.long,\n",
    "                                                        device=model.device),\n",
    "                         max_new_tokens=10000)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1. Evaluate performance in terms of model perplexity (5 points)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
