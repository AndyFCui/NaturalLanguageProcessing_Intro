{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6120 NLP Fall 2023 Assignment 4\n",
    "\n",
    "## About the transformer\n",
    "\n",
    "### Background: \n",
    "- Study the transformer code provided\n",
    "- Apply Transformer code provided in the module to train a language models that generates financial. discourse in Warren Buffet's style. Train you model using Warren Buffets's Annual Letters to shareholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1. Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# check GPU is work\n",
    "# Train the model using a suitable optimizer and loss function.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './emily_dickonson.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\GithubRepo\\NaturalLanguageProcessing_Intro\\as4\\CS6120_NLP_Assignment_4.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GithubRepo/NaturalLanguageProcessing_Intro/as4/CS6120_NLP_Assignment_4.ipynb#W6sZmlsZQ%3D%3D?line=247'>248</a>\u001b[0m         \u001b[39m# inputs_batch is\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GithubRepo/NaturalLanguageProcessing_Intro/as4/CS6120_NLP_Assignment_4.ipynb#W6sZmlsZQ%3D%3D?line=248'>249</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m inputs_batch, targets_batch\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/GithubRepo/NaturalLanguageProcessing_Intro/as4/CS6120_NLP_Assignment_4.ipynb#W6sZmlsZQ%3D%3D?line=251'>252</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39m./emily_dickonson.txt\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GithubRepo/NaturalLanguageProcessing_Intro/as4/CS6120_NLP_Assignment_4.ipynb#W6sZmlsZQ%3D%3D?line=252'>253</a>\u001b[0m     text \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GithubRepo/NaturalLanguageProcessing_Intro/as4/CS6120_NLP_Assignment_4.ipynb#W6sZmlsZQ%3D%3D?line=254'>255</a>\u001b[0m \u001b[39m# text = 'a quick brown fox jumps over the lazy dog.\\n ' \\\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GithubRepo/NaturalLanguageProcessing_Intro/as4/CS6120_NLP_Assignment_4.ipynb#W6sZmlsZQ%3D%3D?line=255'>256</a>\u001b[0m \u001b[39m#        'lazy dog and a quick brown fox.\\n' \\\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GithubRepo/NaturalLanguageProcessing_Intro/as4/CS6120_NLP_Assignment_4.ipynb#W6sZmlsZQ%3D%3D?line=256'>257</a>\u001b[0m \u001b[39m#        'the dog is lazy and the fox jumps quickly.\\n' \\\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GithubRepo/NaturalLanguageProcessing_Intro/as4/CS6120_NLP_Assignment_4.ipynb#W6sZmlsZQ%3D%3D?line=257'>258</a>\u001b[0m \u001b[39m#        'a fox jumps over the dog because he is lazy.\\n' \\\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GithubRepo/NaturalLanguageProcessing_Intro/as4/CS6120_NLP_Assignment_4.ipynb#W6sZmlsZQ%3D%3D?line=258'>259</a>\u001b[0m \u001b[39m#        'dog is lazy and fox is brown. she quickly jumps over the lazy dog.'\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Andy Cui\\.conda\\envs\\nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './emily_dickonson.txt'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "''' Look at all previous tokens to generate next\n",
    "    @Author: Uzair Ahmad\n",
    "    2022\n",
    "    +TransformerBlock \n",
    "'''\n",
    "\n",
    "\n",
    "class TransformerBlockLM(nn.Module):\n",
    "    class TransformerBlock(nn.Module):\n",
    "        def __init__(self, head_count, in_size, out_size):\n",
    "            super().__init__()\n",
    "            self.comm = TransformerBlockLM.MultiHeadAttention(head_count=head_count,\n",
    "                                                              in_size=in_size,\n",
    "                                                              out_size=out_size)\n",
    "            self.think = TransformerBlockLM.MLP(embed_size=out_size)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return x + self.think(x + self.comm(x))\n",
    "\n",
    "    class MLP(nn.Module):\n",
    "        # FFNN (embed_size, embed_size*4, embed_size)\n",
    "        def __init__(self, embed_size):\n",
    "            super().__init__()\n",
    "            self.mlp = nn.Sequential(nn.Linear(embed_size, embed_size * 4),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(embed_size * 4, embed_size))\n",
    "            self.layerNorm = nn.LayerNorm(embed_size)\n",
    "\n",
    "        def forward(self, x):  # think\n",
    "            return self.layerNorm(self.mlp(x))  # paper - after\n",
    "            # return self.mlp(self.layerNorm(x)) # alternate - before\n",
    "\n",
    "    class MultiHeadAttention(nn.Module):\n",
    "        \"\"\"\n",
    "        multiple parallel SA heads (communication among words)\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, head_count, in_size, out_size):\n",
    "            super().__init__()\n",
    "            self.heads = nn.ModuleList(\n",
    "                TransformerBlockLM.SelfAttentionHead(in_size, out_size // head_count)\n",
    "                for _ in range(head_count)\n",
    "            )\n",
    "            self.layerNorm = nn.LayerNorm(out_size)\n",
    "            # self.proj = nn.Linear(out_size, out_size)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # concat over channel/embeddings_size dimension\n",
    "            return self.layerNorm(torch.cat([head(x) for head in self.heads], dim=-1))  # paper - after\n",
    "            # return torch.cat([head(self.layerNorm(x)) for head in self.heads], dim=-1) # alternate - before\n",
    "            # return self.proj(torch.cat([head(x) for head in self.heads], dim=-1))\n",
    "\n",
    "    class SelfAttentionHead(nn.Module):\n",
    "        def __init__(self, in_size, out_size):\n",
    "            \"\"\"\n",
    "            in_size is embed_size\n",
    "            out_size is head_size\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            self.head_size = out_size\n",
    "            self.K = nn.Linear(in_size, self.head_size, bias=False)\n",
    "            self.Q = nn.Linear(in_size, self.head_size, bias=False)\n",
    "            self.V = nn.Linear(in_size, self.head_size, bias=False)\n",
    "\n",
    "        def forward(self, x):\n",
    "            keys = self.K(x)\n",
    "            queries = self.Q(x)\n",
    "            # affinities :\n",
    "            # all the queries will dot-product with all the keys\n",
    "            # transpose (swap) second dimension (input_length) with third (head_size)\n",
    "            keys_t = keys.transpose(1, 2)\n",
    "            autocorrs = (queries @ keys_t) * (self.head_size ** -0.5)  # (batch_size x input_length x input_length)\n",
    "            '''\n",
    "            (batch_size x input_length x embed_size) @ (batch_size x embed_size x input_length) ----> (batch_size x input_length x input_length)\n",
    "            '''\n",
    "            autocorrs = torch.tril(autocorrs)\n",
    "            autocorrs = autocorrs.masked_fill(autocorrs == 0, float('-inf'))\n",
    "            autocorrs = torch.softmax(autocorrs, dim=-1)\n",
    "            values = self.V(x)  # (batch_size x input_length x head_size)\n",
    "            out = autocorrs @ values\n",
    "            return out\n",
    "\n",
    "    def __init__(self, batch_size=4,\n",
    "                 input_length=8,\n",
    "                 embed_size=16,\n",
    "                 sa_head_size=8,\n",
    "                 sa_multihead_count=4,\n",
    "                 pos_embed=False,\n",
    "                 include_mlp=False):\n",
    "        super().__init__()\n",
    "        self.blocks = None\n",
    "        self.ffn = None\n",
    "        self.sa_heads = None\n",
    "        # sa_head_size head_size of self-attention module\n",
    "        self.sa_head_size = sa_head_size\n",
    "        self.sa_multihead_count = sa_multihead_count\n",
    "\n",
    "        self.val_data = None\n",
    "        self.train_data = None\n",
    "        self.val_text = None\n",
    "        self.train_text = None\n",
    "        self.K = None\n",
    "        self.linear_sahead_to_vocab = None\n",
    "        self.vocab = None\n",
    "        self.token_embeddings_table = None\n",
    "        self.vocab_size = None\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.vocab_size: int\n",
    "        self.is_pos_emb = pos_embed\n",
    "        self.include_mlp = include_mlp\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        # input_length = how many consecutive tokens/chars in one input\n",
    "        self.input_length = input_length\n",
    "        # batch_size = how many inputs are going to be processed in-parallel (on GPU)\n",
    "        self.batch_size = batch_size\n",
    "        # embed_size = embedding size\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.lm_head = None\n",
    "        self.position_embeddings_table = None\n",
    "\n",
    "    def forward(self, in_ids, target=None):\n",
    "        in_ids_emb = self.token_embeddings_table(in_ids[:, -self.input_length:])\n",
    "        if self.is_pos_emb:\n",
    "            in_ids_pos_emb = self.position_embeddings_table(\n",
    "                torch.arange(in_ids[:, -self.input_length:].shape[1], device=self.device)\n",
    "            )\n",
    "            in_ids_emb = in_ids_emb + in_ids_pos_emb\n",
    "\n",
    "        block_outputs = self.blocks(in_ids_emb)\n",
    "        logits = self.linear_sahead_to_vocab(block_outputs)  # compute\n",
    "\n",
    "        if target is None:\n",
    "            ce_loss = None\n",
    "        else:\n",
    "            batch_size, input_length, vocab_size = logits.shape\n",
    "            logits_ = logits.view(batch_size * input_length, vocab_size)\n",
    "            targets = target.view(batch_size * input_length)\n",
    "            ce_loss = F.cross_entropy(logits_, targets)\n",
    "        return logits, ce_loss\n",
    "\n",
    "    def fit(self, train_iters=100, eval_iters=10, lr=0.0001):\n",
    "        \"\"\"\n",
    "        train_iters = how many training iterations\n",
    "        eval_iters = how many batches to evaluate to get average performance\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        for iteration in range(train_iters):\n",
    "            if iteration % eval_iters == 0:\n",
    "                avg_loss = self.eval_loss(eval_iters)\n",
    "                print(f\"iter {iteration}: train {avg_loss['train']} val {avg_loss['eval']}\")\n",
    "            inputs, targets = self.get_batch(split='train')\n",
    "            _, ce_loss = self(inputs, targets)\n",
    "            optimizer.zero_grad(set_to_none=True)  # clear gradients of previous step\n",
    "            ce_loss.backward()  # propagate loss back to each unit in the network\n",
    "            optimizer.step()  # update network parameters w.r.t the loss\n",
    "        # torch.save(self, 'sa_pos_')\n",
    "\n",
    "    def generate(self, context_token_ids, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            token_rep, _ = self(context_token_ids)\n",
    "            last_token_rep = token_rep[:, -1, :]\n",
    "            probs = F.softmax(last_token_rep, dim=1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            context_token_ids = torch.cat((context_token_ids, next_token), dim=1)\n",
    "        output_text = self.decoder(context_token_ids[0].tolist())\n",
    "        return output_text\n",
    "\n",
    "    @torch.no_grad()  # tell torch not to prepare for back-propagation (context manager)\n",
    "    def eval_loss(self, eval_iters):\n",
    "        perf = {}\n",
    "        # set dropout and batch normalization layers to evaluation mode before running inference.\n",
    "        self.eval()\n",
    "        for split in ['train', 'eval']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                tokens, targets = self.get_batch(split)  # get random batch of inputs and targete\n",
    "                _, ce_loss = self(tokens, targets)  # forward pass\n",
    "                losses[k] = ce_loss.item()  # the value of loss tensor as a standard Python number\n",
    "            perf[split] = losses.mean()\n",
    "        self.train()  # turn-on training mode-\n",
    "        return perf\n",
    "\n",
    "    def prep(self, corpus):\n",
    "        self.vocab = sorted(list(set(corpus)))\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        c2i = {c: i for i, c in\n",
    "               enumerate(self.vocab)}  # char c to integer i map. assign value i for every word in vocab\n",
    "        i2c = {i: c for c, i in c2i.items()}  # integer i to char c map\n",
    "\n",
    "        self.encoder = lambda doc: [c2i[c] for c in doc]\n",
    "        self.decoder = lambda nums: ''.join([i2c[i] for i in nums])\n",
    "\n",
    "        n = len(text)\n",
    "        self.train_text = text[:int(n * 0.9)]\n",
    "        self.val_text = text[int(n * 0.9):]\n",
    "\n",
    "        self.train_data = torch.tensor(self.encoder(self.train_text), dtype=torch.long)\n",
    "        self.val_data = torch.tensor(self.encoder(self.val_text), dtype=torch.long)\n",
    "\n",
    "        # look-up table for embeddings (vocab_size x embed_size)\n",
    "        # it will be mapping each token id to a vector of embed_size\n",
    "        # a wrapper to store vector representations of each token\n",
    "        self.token_embeddings_table = \\\n",
    "            nn.Embedding(self.vocab_size, self.embed_size)\n",
    "\n",
    "        if self.is_pos_emb:\n",
    "            self.position_embeddings_table = nn.Embedding(self.input_length, self.embed_size)\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
    "                                                in_size=self.embed_size,\n",
    "                                                out_size=self.sa_head_size),\n",
    "        )\n",
    "        # linear projection of sa_head output to vocabulary\n",
    "        self.linear_sahead_to_vocab = nn.Linear(self.sa_head_size, self.vocab_size)\n",
    "\n",
    "    def get_batch(self, split='train'):\n",
    "        data = self.train_data if split == 'train' else self.val_data\n",
    "        # get random chunks of length batch_size from data\n",
    "        ix = torch.randint(len(data) - self.input_length,\n",
    "                           (self.batch_size,))\n",
    "        inputs_batch = torch.stack([data[i:i + self.input_length] for i in ix])\n",
    "        targets_batch = torch.stack([data[i + 1:i + self.input_length + 1] for i in ix])\n",
    "        inputs_batch = inputs_batch.to(self.device)\n",
    "        targets_batch = targets_batch.to(self.device)\n",
    "        # inputs_batch is\n",
    "        return inputs_batch, targets_batch\n",
    "\n",
    "\n",
    "with open('./emily_dickonson.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# text = 'a quick brown fox jumps over the lazy dog.\\n ' \\\n",
    "#        'lazy dog and a quick brown fox.\\n' \\\n",
    "#        'the dog is lazy and the fox jumps quickly.\\n' \\\n",
    "#        'a fox jumps over the dog because he is lazy.\\n' \\\n",
    "#        'dog is lazy and fox is brown. she quickly jumps over the lazy dog.'\n",
    "\n",
    "model = TransformerBlockLM(batch_size=64,\n",
    "                           input_length=32,\n",
    "                           embed_size=128,\n",
    "                           sa_multihead_count=8,\n",
    "                           sa_head_size=128,\n",
    "                           pos_embed=True,\n",
    "                           include_mlp=True)\n",
    "model = model.to(model.device)\n",
    "model.prep(text)\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "print(f'params {sum([np.prod(p.size()) for p in model_parameters])}')\n",
    "input_batch, output_batch = model.get_batch(split='train')\n",
    "_, _ = model(input_batch, output_batch)\n",
    "model.fit(train_iters=4000, eval_iters=1000, lr=1e-3)\n",
    "outputs = model.generate(context_token_ids=torch.zeros((1, 1),\n",
    "                                                        dtype=torch.long,\n",
    "                                                        device=model.device),\n",
    "                         max_new_tokens=1000)\n",
    "print(outputs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1. Identify the points where code is different from the proposed architecture in Google's patent (provided in the module) (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Apply Transformer code provided in the module to train a language models that generates financial discourse in Warren Buffet's style. Train you model using Warren Buffets's Annual Letters to shareholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERKSHIRE HATHAWAY INC. \n",
      "\n",
      "\n",
      "\n",
      "To the Shareholders of Berkshire Hathaway Inc.: \n",
      "\n",
      "Our gain in net worth during 2006 was $16.9 billion, which increased the per-share book value of \n",
      "both our Class A and Class B stock by 18.4%. Over the last 42 years (that is, since present management \n",
      "took over) book value has grown from $19 to $70,281, a rate of 21.4% compounded annually.* \n",
      "\n",
      "We believe that $16.9 billion is a record for a one-year gain in net worth - more than has ever \n",
      "been booked by any American business, leaving aside boosts that have occurred because of mergers (e.g., \n",
      "AOL's purchase of Time Warner). Of course, Exxon Mobil and other companies earn far more than \n",
      "Berkshire, but their earnings largely go to dividends and/or repurchases, rather than to building net worth. \n",
      "\n",
      "All that said, a confession about our 2006 gain is in order. Our most important business, \n",
      "insurance, benefited from a large dose of luck: Mother Nature, bless her heart, went on vacation. After \n",
      "hammering us with hurri\n",
      "<class 'str'>\n",
      "The length of the `data` is 330140\n"
     ]
    }
   ],
   "source": [
    "# Loading and preparing the Warren Buffet dataset\n",
    "with open('WarrenBuffet.txt', 'r') as file:\n",
    "    data = file.read()\n",
    "    \n",
    "print(data[:1000])\n",
    "print(type(data))\n",
    "print('The length of the {} is {}' .format('`data`', len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import package\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from pprint import pprint\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import  word_tokenize\n",
    "import nltk\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "# add more packages\n",
    "from typing import Dict\n",
    "from typing import Set\n",
    "from typing import List\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from math import sqrt\n",
    "from itertools import chain\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pre-processing:\n",
    "- Tokenize the dataset.\n",
    "- Remove stopwords and non-alphabetic tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data\n",
    "def clean_corpus(line: list[str]) -> list[str]:\n",
    "    '''\n",
    "    preprocess and clean a given line.\n",
    "\n",
    "    - line: The text line to be cleaned.\n",
    "    ---\n",
    "    - list: A list of preprocessed tokens from the line.\n",
    "    '''\n",
    "    tokens = word_tokenize(line.lower())\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]  \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26557\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['berkshire',\n",
       " 'hathaway',\n",
       " 'shareholders',\n",
       " 'berkshire',\n",
       " 'hathaway',\n",
       " 'gain',\n",
       " 'net',\n",
       " 'worth',\n",
       " 'billion',\n",
       " 'increased']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing the data\n",
    "lines: list[list[str]] = []\n",
    "\n",
    "# Predefined list of stop words\n",
    "stop_words: set = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenize the text documents and update the lists word_list and lines\n",
    "lines = clean_corpus(data)\n",
    "print(len(lines))\n",
    "lines[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1. Evaluate performance in terms of model perplexity (5 points)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
